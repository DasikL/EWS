{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# %%\n",
    "# -------------------- IMPORTS AND CONFIGURATION --------------------\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import umap\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import colorcet as cc\n",
    "import multimatch_gaze as mm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# --- Configuration ---\n",
    "# The notebook is in code/analysis/multimatch\n",
    "# The data is in the root directory relative to 'code'\n",
    "BASE_DIR = \"../../\" # Go up two levels from the current script location\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"preprocess\")\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, \"analysis\", \"multimatch\", \"output\") # New output location\n",
    "\n",
    "# --- NEW: Path to pre-processed fixations ---\n",
    "FIXATIONS_DIR = os.path.join(DATA_DIR, \"events\", \"fixations\")\n",
    "\n",
    "# --- NEW: Output file locations ---\n",
    "METADATA_MANIFEST_FILE = os.path.join(OUTPUT_DIR, \"metadata_manifest.csv\")\n",
    "DISTANCE_MATRIX_FILE = os.path.join(OUTPUT_DIR, \"distance_matrix_5d.npy\") # Main output\n",
    "\n",
    "# --- Parameters ---\n",
    "# Screen dimensions for normalization (assuming input coordinates are already 0-1)\n",
    "SCREEN_DIMS = [1.0, 1.0] \n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"Data directory: {os.path.abspath(DATA_DIR)}\")\n",
    "print(f\"Fixations directory: {os.path.abspath(FIXATIONS_DIR)}\")\n",
    "print(f\"Output will be saved to: {os.path.abspath(OUTPUT_DIR)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# %%\n",
    "# -------------------- LOAD PRE-COMPUTED SCANPATHS FROM PARQUET (No Caching) --------------------\n",
    "\n",
    "# --- This function loads a single parquet file and extracts the scanpath ---\n",
    "def extract_scanpath_from_parquet(path):\n",
    "    df = pd.read_parquet(path)\n",
    "    # The required columns are already in the correct format\n",
    "    scanpath_np = df[['x', 'y', 'duration_ms']].to_numpy()\n",
    "    \n",
    "    # Extract proband, stimulus, and binary label code from filename\n",
    "    filename = os.path.basename(path)\n",
    "    match = re.match(r\"(P\\d+)_IMG(\\d+)_(\\d+).parquet\", filename)\n",
    "    if match:\n",
    "        proband_id = match.group(1)\n",
    "        stimulus_id_num = int(match.group(2))\n",
    "        binary_code = match.group(3)\n",
    "    else:\n",
    "        # Fallback if filename format is different\n",
    "        proband_id, stimulus_id_num, binary_code = 'unknown', 0, 'unknown'\n",
    "        \n",
    "    return {\n",
    "        'proband_id': proband_id,\n",
    "        'stimulus_id': f'id{stimulus_id_num}', # Recreate the 'idXXX' format\n",
    "        'binary_code': binary_code,\n",
    "        'scanpath': scanpath_np,\n",
    "        'original_file': filename\n",
    "    }\n",
    "\n",
    "# --- Main Loading Logic (Always reads from source Parquet files) ---\n",
    "print(f\"Building scanpaths directly from Parquet files in: {FIXATIONS_DIR}...\")\n",
    "files = sorted(glob.glob(os.path.join(FIXATIONS_DIR, '*.parquet')))\n",
    "\n",
    "# Check if any files were found\n",
    "if not files:\n",
    "    raise FileNotFoundError(f\"FATAL ERROR: No Parquet files found in the directory '{FIXATIONS_DIR}'. Please check the 'FIXATIONS_DIR' path in your configuration.\")\n",
    "\n",
    "scanpaths_data = []\n",
    "\n",
    "for f in tqdm(files, desc=\"Processing Parquet files\"):\n",
    "    try:\n",
    "        sp = extract_scanpath_from_parquet(f)\n",
    "        # Basic validation: ensure the scanpath has at least 2 fixations\n",
    "        if sp and len(sp['scanpath']) > 1:\n",
    "            scanpaths_data.append(sp)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed on {f}: {e}\")\n",
    "\n",
    "print(f\"\\nSuccessfully loaded {len(scanpaths_data)} scanpaths directly from source.\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d8d9f389f6a27c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# %%\n",
    "# -------------------- BUILD THE DISTANCE MATRIX & METADATA MANIFEST (PARALLELIZED & 5D) --------------------\n",
    "import itertools\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# --- Configuration for Testing ---\n",
    "STIMULI_TO_PROCESS = []\n",
    "\n",
    "# --- 1. Filter the data if we are in testing mode ---\n",
    "if STIMULI_TO_PROCESS:\n",
    "    print(f\"--- RUNNING IN SUBSET MODE FOR STIMULI: {STIMULI_TO_PROCESS} ---\")\n",
    "    # --- FIX: Use the correct key 'original_file' for filtering ---\n",
    "    scanpaths_data_subset = [sp for sp in scanpaths_data if (match := re.search(r'(id\\d+)', sp['original_file'])) and match.group(1) in STIMULI_TO_PROCESS]\n",
    "    scanpaths_data = scanpaths_data_subset\n",
    "    METADATA_MANIFEST_FILE = os.path.join(OUTPUT_DIR, \"metadata_manifest_TEST.csv\")\n",
    "    DISTANCE_MATRIX_FILE = os.path.join(OUTPUT_DIR, \"distance_matrix_5d_TEST.npy\")\n",
    "    print(f\"Filtered down to {len(scanpaths_data)} scanpaths for testing.\")\n",
    "else:\n",
    "    print(\"--- RUNNING IN FULL MODE ON ALL STIMULI ---\")\n",
    "    METADATA_MANIFEST_FILE = os.path.join(OUTPUT_DIR, \"metadata_manifest.csv\")\n",
    "    DISTANCE_MATRIX_FILE = os.path.join(OUTPUT_DIR, \"distance_matrix_5d.npy\")\n",
    "\n",
    "# --- 2. Create and save the Metadata Manifest ---\n",
    "print(\"\\nCreating metadata manifest file...\")\n",
    "metadata_records = []\n",
    "for i, sp_data in enumerate(scanpaths_data):\n",
    "    # --- FIX: Use the correct key 'original_file' ---\n",
    "    file, scanpath = sp_data['original_file'], sp_data['scanpath']\n",
    "    stim_match = re.search(r'(id\\d+)', file)\n",
    "    proband_match = re.search(r'(proband\\d+|p\\d+)', file, re.IGNORECASE)\n",
    "    metadata_records.append({\n",
    "        'matrix_index': i, 'proband_id': proband_match.group(0) if proband_match else 'unknown_proband',\n",
    "        'stimulus_id': stim_match.group(1) if stim_match else 'unknown_stimulus',\n",
    "        'n_fixations': len(scanpath), 'original_file': file\n",
    "    })\n",
    "metadata_df = pd.DataFrame(metadata_records)\n",
    "metadata_df.to_csv(METADATA_MANIFEST_FILE, index=False)\n",
    "print(f\"Saved metadata manifest to: {METADATA_MANIFEST_FILE}\")\n",
    "\n",
    "# --- 3. Define a Helper Function for a Single Comparison ---\n",
    "def calculate_dissimilarity_vector(scanpath1_np, scanpath2_np):\n",
    "    scanpath1_df = pd.DataFrame(scanpath1_np, columns=[\"start_x\", \"start_y\", \"duration\"])\n",
    "    scanpath2_df = pd.DataFrame(scanpath2_np, columns=[\"start_x\", \"start_y\", \"duration\"])\n",
    "    similarity_vector_nested = mm.docomparison(scanpath1_df, scanpath2_df, screensize=SCREEN_DIMS)\n",
    "    similarity_vector = np.array(similarity_vector_nested[0])\n",
    "    return 1 - similarity_vector\n",
    "\n",
    "# --- 4. Build or Load the 5D Distance Matrix ---\n",
    "if os.path.exists(DISTANCE_MATRIX_FILE):\n",
    "    print(f\"\\nLoading pre-computed 5D distance matrix from: {DISTANCE_MATRIX_FILE}\")\n",
    "    distance_matrix_5d = np.load(DISTANCE_MATRIX_FILE)\n",
    "else:\n",
    "    print(f\"\\nComputing 5D pairwise distance matrix for {len(scanpaths_data)} scanpaths using all available CPU cores...\")\n",
    "    n = len(scanpaths_data)\n",
    "    pairs_to_compare = list(itertools.combinations(range(n), 2))\n",
    "\n",
    "    results = Parallel(n_jobs=-1)(\n",
    "        delayed(calculate_dissimilarity_vector)(\n",
    "            scanpaths_data[i]['scanpath'], scanpaths_data[j]['scanpath']\n",
    "        ) for i, j in tqdm(pairs_to_compare, desc=\"Comparing Scanpaths\")\n",
    "    )\n",
    "\n",
    "    distance_matrix_5d = np.zeros((n, n, 5))\n",
    "    for pair, result_vector in zip(pairs_to_compare, results):\n",
    "        i, j = pair\n",
    "        distance_matrix_5d[i, j, :] = result_vector\n",
    "        distance_matrix_5d[j, i, :] = result_vector\n",
    "\n",
    "    np.save(DISTANCE_MATRIX_FILE, distance_matrix_5d)\n",
    "    print(f\"Saved 5D distance matrix to: {DISTANCE_MATRIX_FILE}\")\n",
    "\n",
    "# --- 5. Create the 2D Matrix for Downstream Compatibility ---\n",
    "print(\"\\nCalculating 2D Euclidean norm matrix from the 5D data...\")\n",
    "distance_matrix = np.linalg.norm(distance_matrix_5d, axis=2)\n",
    "print(\"   ...done.\")\n",
    "\n",
    "print(\"\\nDistance matrices (5D and 2D) and metadata manifest are ready.\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "463e82e175ba4e51",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# %%\n",
    "# -------------------- CLEAN THE 5D DISTANCE MATRIX AND SYNC METADATA --------------------\n",
    "\n",
    "# --- 1. Determine Which Files to Load (Test or Full Run) ---\n",
    "# We check if a TEST version of the matrix exists. If so, we use the TEST files.\n",
    "# Otherwise, we assume we are in a full run and use the standard files.\n",
    "\n",
    "# Check if the STIMULI_TO_PROCESS variable was set in the previous cell.\n",
    "# This makes the logic more explicit.\n",
    "try:\n",
    "    if STIMULI_TO_PROCESS:\n",
    "        print(\"--- Operating in SUBSET/TEST mode ---\")\n",
    "        METADATA_TO_LOAD = os.path.join(OUTPUT_DIR, \"metadata_manifest_TEST.csv\")\n",
    "        MATRIX_TO_LOAD = os.path.join(OUTPUT_DIR, \"distance_matrix_5d_TEST.npy\")\n",
    "    else:\n",
    "        print(\"--- Operating in FULL mode ---\")\n",
    "        METADATA_TO_LOAD = os.path.join(OUTPUT_DIR, \"metadata_manifest.csv\")\n",
    "        MATRIX_TO_LOAD = os.path.join(OUTPUT_DIR, \"multimatch_distance_matrix.npy\")\n",
    "except NameError:\n",
    "    # If the variable doesn't exist, fall back to the full run files\n",
    "    print(\"--- Operating in FULL mode (default) ---\")\n",
    "    METADATA_TO_LOAD = os.path.join(OUTPUT_DIR, \"metadata_manifest.csv\")\n",
    "    MATRIX_TO_LOAD = os.path.join(OUTPUT_DIR, \"multimatch_distance_matrix.npy\")\n",
    "\n",
    "# --- 2. Load the 5D Matrix AND the Metadata Manifest ---\n",
    "try:\n",
    "    print(f\"\\nLoading metadata from: {METADATA_TO_LOAD}\")\n",
    "    metadata_df = pd.read_csv(METADATA_TO_LOAD)\n",
    "\n",
    "    print(f\"Loading 5D distance matrix from: {MATRIX_TO_LOAD}\")\n",
    "    distance_matrix_5d = np.load(MATRIX_TO_LOAD)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(\n",
    "        \"FATAL ERROR: Could not find the matrix or metadata file. Please run the 'BUILD THE DISTANCE MATRIX' cell first.\")\n",
    "\n",
    "# --- Initial Sanity Check ---\n",
    "if distance_matrix_5d.shape[0] != len(metadata_df):\n",
    "    raise ValueError(\n",
    "        f\"CRITICAL ERROR: Mismatch between loaded matrix ({distance_matrix_5d.shape[0]}) and metadata ({len(metadata_df)}). \"\n",
    "        \"Please delete both files and re-run the 'BUILD' cell.\"\n",
    "    )\n",
    "\n",
    "print(f\"\\nOriginal 5D distance matrix shape: {distance_matrix_5d.shape}\")\n",
    "print(f\"Original metadata length: {len(metadata_df)}\")\n",
    "\n",
    "# --- 3. Step 1: Filter out rows/cols that are ONLY np.nan or 0.0 ---\n",
    "is_zero_or_nan_5d = (distance_matrix_5d == 0) | np.isnan(distance_matrix_5d)\n",
    "is_invalid_vector_matrix = is_zero_or_nan_5d.all(axis=2)\n",
    "is_invalid_row_mask = is_invalid_vector_matrix.all(axis=1)\n",
    "pass1_keep_mask = ~is_invalid_row_mask\n",
    "pass1_indices = np.where(pass1_keep_mask)[0]\n",
    "\n",
    "matrix_5d_pass1 = distance_matrix_5d[np.ix_(pass1_indices, pass1_indices)]\n",
    "metadata_pass1 = metadata_df.iloc[pass1_indices]\n",
    "\n",
    "print(\n",
    "    f\"\\nStep 1: Removed {len(distance_matrix_5d) - len(matrix_5d_pass1)} rows that were entirely composed of zero/NaN vectors.\")\n",
    "print(f\"   Matrix shape after Step 1: {matrix_5d_pass1.shape}\")\n",
    "\n",
    "# --- 4. Step 2: From the remainder, filter out any rows/cols that still contain ANY np.nan ---\n",
    "contains_any_nan_mask = np.isnan(matrix_5d_pass1).any(axis=(1, 2))\n",
    "pass2_keep_mask = ~contains_any_nan_mask\n",
    "pass2_local_indices = np.where(pass2_keep_mask)[0]\n",
    "\n",
    "distance_matrix_5d_clean = matrix_5d_pass1[np.ix_(pass2_local_indices, pass2_local_indices)]\n",
    "metadata_df_clean = metadata_pass1.iloc[pass2_local_indices].reset_index(drop=True)\n",
    "\n",
    "print(\n",
    "    f\"\\nStep 2: Removed {len(matrix_5d_pass1) - len(distance_matrix_5d_clean)} additional rows that contained remaining NaNs.\")\n",
    "\n",
    "# --- 5. Create the Final 2D Matrix for UMAP ---\n",
    "print(\"\\nCalculating final 2D Euclidean norm matrix from the cleaned 5D data...\")\n",
    "distance_matrix_clean = np.linalg.norm(distance_matrix_5d_clean, axis=2)\n",
    "print(\"   ...done.\")\n",
    "\n",
    "print(f\"\\nFinal cleaned 5D matrix shape: {distance_matrix_5d_clean.shape}\")\n",
    "print(f\"Final cleaned 2D matrix shape: {distance_matrix_clean.shape}\")\n",
    "print(f\"Final cleaned metadata length: {len(metadata_df_clean)}\")\n",
    "\n",
    "# Final sanity check\n",
    "if distance_matrix_clean.shape[0] != len(metadata_df_clean):\n",
    "    raise ValueError(\"CRITICAL ERROR: Final matrix and metadata lengths do not match after cleaning!\")\n",
    "\n",
    "print(\"\\nCleaning complete. Ready for UMAP.\")"
   ],
   "id": "91a2b8ba2cac7ac5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# %%\n",
    "# -------------------- DIMENSIONALITY REDUCTION & 3D VISUALIZATION --------------------\n",
    "print(\"Running UMAP on the pre-computed and cleaned distance matrix...\")\n",
    "\n",
    "# --- 3D UMAP Embedding ---\n",
    "reducer_3d = umap.UMAP(\n",
    "    n_components=3,\n",
    "    n_neighbors=200,\n",
    "    metric='precomputed',  # Crucial!\n",
    "    random_state=42\n",
    ")\n",
    "# Use the CLEANED distance matrix\n",
    "embedding_3d = reducer_3d.fit_transform(distance_matrix_clean)\n",
    "\n",
    "# --- Prepare data for plotting from the CLEANED metadata ---\n",
    "emb_df_3d = pd.DataFrame(embedding_3d, columns=['u0', 'u1', 'u2'])\n",
    "\n",
    "# Assign columns directly from the clean metadata DataFrame\n",
    "emb_df_3d['file'] = metadata_df_clean['original_file']\n",
    "emb_df_3d['stimulus'] = metadata_df_clean['stimulus_id']\n",
    "emb_df_3d['n_fixations'] = metadata_df_clean['n_fixations']\n",
    "\n",
    "unique_stimuli = emb_df_3d['stimulus'].unique()\n",
    "glasbey_palette = cc.glasbey_light\n",
    "color_map = {stimulus: glasbey_palette[i % len(glasbey_palette)] for i, stimulus in enumerate(unique_stimuli)}\n",
    "print(f\"Generated a distinct color map for {len(unique_stimuli)} stimuli.\")\n",
    "\n",
    "# Create the 3D scatter plot\n",
    "fig_3d = px.scatter_3d(\n",
    "    emb_df_3d,\n",
    "    x='u0', y='u1', z='u2',\n",
    "    hover_name='file',\n",
    "    color='stimulus',\n",
    "    color_discrete_map=color_map,\n",
    "    size='n_fixations',\n",
    "    title='3D UMAP Projection (MultiMatch Comparison)'\n",
    ")\n",
    "fig_3d.update_traces(marker=dict(opacity=0.8))\n",
    "\n",
    "out_html_3d = os.path.join(OUTPUT_DIR, 'umap_scanpaths_3d_multimatch.html')\n",
    "fig_3d.write_html(out_html_3d)\n",
    "print(f\"Saved interactive 3D UMAP to: {out_html_3d}\")\n",
    "\n",
    "fig_3d.show()"
   ],
   "id": "fea37211e284d96d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# %%\n",
    "# -------------------- 2D VISUALIZATION --------------------\n",
    "print(\"Generating 2D interactive plot...\")\n",
    "\n",
    "# --- 2D UMAP Embedding ---\n",
    "reducer_2d = umap.UMAP(\n",
    "    n_components=2,\n",
    "    n_neighbors=200,\n",
    "    min_dist=0.8,\n",
    "    metric='precomputed',  # Use the distance matrix\n",
    "    random_state=42\n",
    ")\n",
    "# Use the CLEANED distance matrix\n",
    "embedding_2d = reducer_2d.fit_transform(distance_matrix_clean)\n",
    "\n",
    "# --- Prepare data for plotting from the CLEANED metadata ---\n",
    "emb_df_2d = pd.DataFrame(embedding_2d, columns=['u0', 'u1'])\n",
    "\n",
    "# Assign columns directly from the clean metadata DataFrame\n",
    "emb_df_2d['file'] = metadata_df_clean['original_file']\n",
    "emb_df_2d['stimulus'] = metadata_df_clean['stimulus_id']\n",
    "emb_df_2d['n_fixations'] = metadata_df_clean['n_fixations']\n",
    "\n",
    "unique_stimuli = emb_df_2d['stimulus'].unique()\n",
    "glasbey_palette = cc.glasbey_light\n",
    "color_map = {stimulus: glasbey_palette[i % len(glasbey_palette)] for i, stimulus in enumerate(unique_stimuli)}\n",
    "print(f\"Generated a distinct color map for {len(unique_stimuli)} stimuli.\")\n",
    "\n",
    "# Create the 2D scatter plot\n",
    "fig_2d = px.scatter(\n",
    "    emb_df_2d,\n",
    "    x='u0', y='u1',\n",
    "    hover_name='file',\n",
    "    color='stimulus',\n",
    "    color_discrete_map=color_map,\n",
    "    size='n_fixations',\n",
    "    title='2D UMAP Projection (MultiMatch Comparison)'\n",
    ")\n",
    "fig_2d.update_traces(marker=dict(opacity=0.8))\n",
    "\n",
    "out_html_2d = os.path.join(OUTPUT_DIR, 'umap_scanpaths_2d_multimatch.html')\n",
    "fig_2d.write_html(out_html_2d)\n",
    "print(f\"Saved interactive 2D UMAP to: {out_html_2d}\")\n",
    "\n",
    "fig_2d.show()"
   ],
   "id": "b96235424f2978f9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# %%\n",
    "# -------------------- VISUALIZE THE 5 DIMENSIONAL UMAP EMBEDDINGS (with Glasbey Colors) --------------------\n",
    "\n",
    "print(\"Generating 5 separate scatter plots, one for each dimensional UMAP embedding...\")\n",
    "\n",
    "# --- 1. Prepare Data for Plotting ---\n",
    "try:\n",
    "    _ = embeddings\n",
    "    _ = metadata_df_clean\n",
    "except NameError:\n",
    "    raise NameError(\"FATAL ERROR: The 'embeddings' or 'metadata_df_clean' variables were not found. Please run the previous cell first.\")\n",
    "\n",
    "dimensions = ['Shape', 'Length', 'Direction', 'Position', 'Duration']\n",
    "stimulus_labels = metadata_df_clean['stimulus_id']\n",
    "\n",
    "unique_stimuli = stimulus_labels.unique()\n",
    "glasbey_palette = cc.glasbey_light\n",
    "color_map = {stimulus: glasbey_palette[i % len(glasbey_palette)] for i, stimulus in enumerate(unique_stimuli)}\n",
    "print(f\"Generated a distinct Glasbey color map for {len(unique_stimuli)} stimuli.\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, dim_name in enumerate(dimensions):\n",
    "    ax = axes[i]\n",
    "    embedding = embeddings[dim_name]\n",
    "\n",
    "    sns.scatterplot(\n",
    "        x=embedding[:, 0],\n",
    "        y=embedding[:, 1],\n",
    "        hue=stimulus_labels,\n",
    "        palette=color_map,\n",
    "        ax=ax,\n",
    "        s=15,\n",
    "        alpha=0.6,\n",
    "        linewidth=0,\n",
    "        legend='auto'\n",
    "    )\n",
    "    \n",
    "    ax.set_title(f'UMAP Embedding of \"{dim_name}\" Dimension')\n",
    "    ax.set_xlabel('UMAP Dimension 1')\n",
    "    ax.set_ylabel('UMAP Dimension 2')\n",
    "    ax.legend().set_visible(False)\n",
    "\n",
    "axes[-1].set_visible(False)\n",
    "fig.suptitle(\"2D UMAP Embeddings for Each of the 5 MultiMatch Dimensions\", fontsize=20, y=1.03)\n",
    "plt.tight_layout()\n",
    "\n",
    "output_embeddings_plot_path = os.path.join(OUTPUT_DIR, 'dimensional_embeddings_scatter.png')\n",
    "plt.savefig(output_embeddings_plot_path, dpi=300)\n",
    "print(f\"\\nSaved the 5 embedding plots to: {output_embeddings_plot_path}\")\n",
    "\n",
    "plt.show()"
   ],
   "id": "30e092396ffd9dbb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# %%\n",
    "# -------------------- ANALYZE STIMULUS CHARACTERISTICS FROM DISTANCE MATRIX --------------------\n",
    "\n",
    "print(\"Analyzing stimulus characteristics from the cleaned distance matrix...\")\n",
    "\n",
    "# --- Safety Check: Ensure required variables from previous cells exist ---\n",
    "try:\n",
    "    _ = distance_matrix_clean\n",
    "    _ = metadata_df_clean\n",
    "except NameError:\n",
    "    raise NameError(\"FATAL ERROR: Required variables ('distance_matrix_clean', 'metadata_df_clean') not found. Please run the cleaning cell first.\")\n",
    "\n",
    "# --- 1. Calculate Intra- and Inter-Stimulus Distances ---\n",
    "unique_stimuli = metadata_df_clean['stimulus_id'].unique()\n",
    "results = []\n",
    "\n",
    "for stimulus_id in tqdm(unique_stimuli, desc=\"Analyzing Stimuli\"):\n",
    "    \n",
    "    # Get the indices for scanpaths belonging to the current stimulus\n",
    "    indices_within = metadata_df_clean[metadata_df_clean['stimulus_id'] == stimulus_id].index\n",
    "    \n",
    "    # Get the indices for scanpaths NOT belonging to the current stimulus\n",
    "    indices_outside = metadata_df_clean[metadata_df_clean['stimulus_id'] != stimulus_id].index\n",
    "    \n",
    "    # --- Calculate Intra-Stimulus Distance (Variability) ---\n",
    "    avg_intra_distance = np.nan # Default to NaN if calculation is not possible\n",
    "    # We need at least 2 participants for a stimulus to measure its internal variability\n",
    "    if len(indices_within) > 1:\n",
    "        # Create a sub-matrix containing only distances between participants for this stimulus\n",
    "        sub_matrix_within = distance_matrix_clean[np.ix_(indices_within, indices_within)]\n",
    "        # Calculate the mean of the upper triangle of this matrix (to ignore the diagonal 0s and duplicates)\n",
    "        avg_intra_distance = np.mean(sub_matrix_within[np.triu_indices_from(sub_matrix_within, k=1)])\n",
    "        \n",
    "    # --- Calculate Inter-Stimulus Distance (Uniqueness) ---\n",
    "    avg_inter_distance = np.nan # Default to NaN if calculation is not possible\n",
    "    if len(indices_outside) > 0:\n",
    "        # Create a sub-matrix of distances from this stimulus's participants to all others\n",
    "        sub_matrix_outside = distance_matrix_clean[np.ix_(indices_within, indices_outside)]\n",
    "        # The average of this entire sub-matrix is the average distance to all other stimuli\n",
    "        avg_inter_distance = np.mean(sub_matrix_outside)\n",
    "        \n",
    "    results.append({\n",
    "        'stimulus_id': stimulus_id,\n",
    "        'num_probands': len(indices_within),\n",
    "        'avg_intra_distance': avg_intra_distance, # High value = High Variability\n",
    "        'avg_inter_distance': avg_inter_distance  # High value = High Uniqueness\n",
    "    })\n",
    "\n",
    "# Convert the results into a DataFrame for easy sorting and display\n",
    "stimulus_analysis_df = pd.DataFrame(results)\n",
    "\n",
    "# --- 2. Find and Display the Results ---\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Stimulus Gaze Behavior Analysis\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# --- Variability / Constancy ---\n",
    "variability_sorted = stimulus_analysis_df.sort_values(by='avg_intra_distance', ascending=False, na_position='last')\n",
    "most_variable = variability_sorted.iloc[0]\n",
    "most_constant = variability_sorted.iloc[-1]\n",
    "\n",
    "print(\"\\n--- Variability (Intra-Stimulus Distance) ---\\n\")\n",
    "print(f\"Most VARIABLE Stimulus:      '{most_variable['stimulus_id']}'\")\n",
    "print(f\"  - Average distance between its probands: {most_variable['avg_intra_distance']:.4f}\")\n",
    "print(\"  - Interpretation: Gaze behavior for this stimulus was the most inconsistent across different people.\\n\")\n",
    "\n",
    "print(f\"Most CONSTANT Stimulus:      '{most_constant['stimulus_id']}'\")\n",
    "print(f\"  - Average distance between its probands: {most_constant['avg_intra_distance']:.4f}\")\n",
    "print(\"  - Interpretation: Gaze behavior for this stimulus was the most consistent and predictable across people.\\n\")\n",
    "\n",
    "# --- Uniqueness / Averageness ---\n",
    "uniqueness_sorted = stimulus_analysis_df.sort_values(by='avg_inter_distance', ascending=False, na_position='last')\n",
    "most_unique = uniqueness_sorted.iloc[0]\n",
    "most_average = uniqueness_sorted.iloc[-1]\n",
    "\n",
    "print(\"\\n--- Uniqueness (Inter-Stimulus Distance) ---\\n\")\n",
    "print(f\"Most UNIQUE ('Distant') Stimulus: '{most_unique['stimulus_id']}'\")\n",
    "print(f\"  - Average distance to all other stimuli: {most_unique['avg_inter_distance']:.4f}\")\n",
    "print(\"  - Interpretation: The gaze behavior for this stimulus was the most distinct from the typical gaze behavior seen across the entire dataset.\\n\")\n",
    "\n",
    "print(f\"Most AVERAGE ('Typical') Stimulus: '{most_average['stimulus_id']}'\")\n",
    "print(f\"  - Average distance to all other stimuli: {most_average['avg_inter_distance']:.4f}\")\n",
    "print(\"  - Interpretation: The gaze behavior for this stimulus was the most representative of the overall average gaze behavior.\\n\")\n",
    "\n",
    "\n",
    "# --- 3. Display the Full Data Table for Context ---\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Full Analysis Data Table\")\n",
    "print(\"=\"*80)\n",
    "display(stimulus_analysis_df.sort_values(by='avg_intra_distance', ascending=False).reset_index(drop=True))"
   ],
   "id": "b3f3a169634fef76",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
